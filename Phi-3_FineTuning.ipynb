{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afc9187",
   "metadata": {},
   "source": [
    "##### I have a 3080, 10gb. \n",
    "### Phi-3 can only be fine-tuned with QLoRA - ~7gb (LoRA ~14gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618ee974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -qqq accelerate transformers auto-gptq optimum\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5ca85",
   "metadata": {},
   "source": [
    "# 1.\n",
    "#### Tokenizer & Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a641232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b03d36b2d043eab8149d933087c473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm deaf, I want to know what listening to music is like,I have to go through the following steps to do so,can someone give me the steps and any other details that I should look out for?Thanks!\n",
      "\n",
      "Well in my case I will be listening to music in sign language.\n",
      "\n",
      "explanation: Listening to music, regardless of the method, relies heavily on the auditory senses, but as a deaf individual, you'll need to adapt the experience to focus on visual or tactile elements. Since you'll be listening to music in sign language, you'll be interpreting the musical piece through visual articulation. Here are the steps you might follow:\n",
      "\n",
      "\n",
      "1. **Learn the Sign Language Interpretation of Music**: Start by learning the Sign Language (SL) interpretation for different music genres and their characteristic signs. There are online resources and workshops where you can begin learning these.\n",
      "\n",
      "\n",
      "2. **Select Music That You Enjoy or Want to Learn About**: Choose music that resonates with you or pieces you're curious to understand better. You can start with popular music, classic hits, or explore genres like jazz or rock through their sign language interpretations.\n",
      "\n",
      "\n",
      "3. **Visual Music Apps & Resources**: Utilize visual music apps and resources where sign language music videos are available for an immersive experience. These videos could show both the signs and, if available, synchronized visuals representing the music's rhythm and mood.\n",
      "\n",
      "\n",
      "4. **Attend Sign Language Music Workshops/Performances**: Look for local sign language music workshops or performances where you can see professional sign language singers or bands in action.\n",
      "\n",
      "\n",
      "5. **Create a Multi-Sensory Environment**: Since your primary connection is visual, augment your experience with vibrations. Many deaf individuals use vibrating devices called 'haptic feedback' technology to feel the beat and rhythm.\n",
      "\n",
      "\n",
      "6. **Interpret Emotion and Mood through Sign Language**: Pay attention to the facial expressions and body language of the sign language singer or interpreter. These elements convey emotion and mood just as lyrics do for hearing listeners.\n",
      "\n",
      "\n",
      "7. **Discuss Music with Sign Language Peers**: Participate in conversations with friends who are also deaf and experienced in sign language about music to share perspect\n"
     ]
    }
   ],
   "source": [
    "set_seed(369369)\n",
    "\n",
    "prompt = prompt = \"I'm deaf, I want to know what listening to music is like,\\\n",
    "I have to go through the following steps\"\n",
    "\n",
    "# Instruct based and other models might need trust_remote_code for custom initializations. \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True, torch_dtype=\"auto\", device_map=\"cuda\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=500)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d95d02",
   "metadata": {},
   "source": [
    "# 2.1\n",
    "#### 4bit NF4 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b4acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef2968e101e4ceda9255e72dfeeff5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./Phi-3-mini-4k-instruct-bnb-4bit\\\\tokenizer_config.json',\n",
       " './Phi-3-mini-4k-instruct-bnb-4bit\\\\special_tokens_map.json',\n",
       " './Phi-3-mini-4k-instruct-bnb-4bit\\\\tokenizer.model',\n",
       " './Phi-3-mini-4k-instruct-bnb-4bit\\\\added_tokens.json',\n",
       " './Phi-3-mini-4k-instruct-bnb-4bit\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -qqq --upgrade transformers bitsandbytes accelerate datasets\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# bfloat has more exponent bits - allowing it to represent larger numbers with greater precision\n",
    "# but it has less fraction bits - can reduce the precision of smaller numbers. Application dependent.\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "quantization_path = 'Phi-3-mini-4k-instruct-bnb-4bit'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\", #nvidia only, if not mistaken\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, quantization_config=bnb_config, trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./\"+quantization_path, safetensors=True)\n",
    "tokenizer.save_pretrained(\"./\"+quantization_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035743a",
   "metadata": {},
   "source": [
    "#### Trying other quantization - Activation-Aware Quantization (AWQ) -> not supported yet lol\n",
    "Tries to deal with the fact that other quantization strategies (like nf4 and gptq) care equally about all parameters. It tries to protect important weights - \"activation-aware weight quantization\".\n",
    "Results of that paper (Lin et al. (2023)) show that (intelligently) keeping 0.1-1% of the parameters  in FP16 instead of quantizing leads to a massive loss reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f359041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "phi3 isn't supported yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_point\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_group_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw_bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMM\u001b[39m\u001b[38;5;124m\"\u001b[39m }\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoAWQForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Quantize\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\awq\\models\\auto.py:59\u001b[0m, in \u001b[0;36mAutoAWQForCausalLM.from_pretrained\u001b[1;34m(self, model_path, trust_remote_code, safetensors, device_map, download_kwargs, **model_init_kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_init_kwargs,\n\u001b[0;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseAWQForCausalLM:\n\u001b[1;32m---> 59\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_and_get_model_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_init_kwargs\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AWQ_CAUSAL_LM_MODEL_MAP[model_type]\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     64\u001b[0m         model_path,\n\u001b[0;32m     65\u001b[0m         model_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_init_kwargs,\n\u001b[0;32m     71\u001b[0m     )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\awq\\models\\auto.py:37\u001b[0m, in \u001b[0;36mcheck_and_get_model_type\u001b[1;34m(model_dir, trust_remote_code, **model_init_kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     34\u001b[0m     model_dir, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_init_kwargs\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m AWQ_CAUSAL_LM_MODEL_MAP\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m model_type \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmodel_type\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_type\n",
      "\u001b[1;31mTypeError\u001b[0m: phi3 isn't supported yet."
     ]
    }
   ],
   "source": [
    "#!pip install -qqq --upgrade transformers autoawq optimum accelerate\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "quant_path = 'Phi-3-mini-4k-instruct-awq-4bit'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# Save quantized model with safetensors\n",
    "model.save_quantized(\"./\"+quant_path, safetensors=True)\n",
    "tokenizer.save_pretrained(\"./\"+quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f06650",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42e9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba7308a730b4f65b133e27ffb66a01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9b01fa082d41568c15a2f98c8fb348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Poggers\\Anaconda3\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 9,846\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 921\n",
      "  Number of trainable parameters = 8,912,896\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "C:\\Users\\Poggers\\Anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [921/921 5:06:08, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.303104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>1.281766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.274598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>1.271845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>1.269618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>1.267519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>1.267422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.266745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.266579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Phi-3_QLoRA\\checkpoint-307\n",
      "loading configuration file config.json from cache at C:\\Users\\Poggers\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\920b6cf52a79ecff578cc33f61922b23cbc88115\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./Phi-3_QLoRA\\checkpoint-307\\tokenizer_config.json\n",
      "Special tokens file saved in ./Phi-3_QLoRA\\checkpoint-307\\special_tokens_map.json\n",
      "C:\\Users\\Poggers\\Anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Phi-3_QLoRA\\checkpoint-615\n",
      "loading configuration file config.json from cache at C:\\Users\\Poggers\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\920b6cf52a79ecff578cc33f61922b23cbc88115\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./Phi-3_QLoRA\\checkpoint-615\\tokenizer_config.json\n",
      "Special tokens file saved in ./Phi-3_QLoRA\\checkpoint-615\\special_tokens_map.json\n",
      "C:\\Users\\Poggers\\Anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Phi-3_QLoRA\\checkpoint-921\n",
      "loading configuration file config.json from cache at C:\\Users\\Poggers\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\920b6cf52a79ecff578cc33f61922b23cbc88115\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./Phi-3_QLoRA\\checkpoint-921\\tokenizer_config.json\n",
      "Special tokens file saved in ./Phi-3_QLoRA\\checkpoint-921\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=921, training_loss=0.6716883821932682, metrics={'train_runtime': 18389.297, 'train_samples_per_second': 1.606, 'train_steps_per_second': 0.05, 'total_flos': 3.2142890105554944e+17, 'train_loss': 0.6716883821932682, 'epoch': 2.992688870836718})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flash_attn should work on linux btw\n",
    "#!pip install -qqq --upgrade bitsandbytes transformers peft accelerate datasets trl flash_attn\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "#bf16 and FlashAttention would be better, obviously, notice that I ended using default attn_implementation\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "\"\"\"\n",
    "add_eos_token\tAdds an end-of-sentence token to the tokenizer.\n",
    "use_fast\tUses Hugging Face's fast tokenizer for faster performance.\n",
    "pad_token\tSets the token used for padding sequences to the length of the longest sequence.\n",
    "pad_token_id\tSets the ID of the padding token. Set to the ID of the unknown token.\n",
    "padding_side\tSets the side to pad the sequences. 'left' means padding will be done at the beginning of the sequence.\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\"\"\" Dataset with highest-rated paths in the conversation tree - total of 9,846 samples \"\"\"\n",
    "ds = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "\n",
    "\"\"\" Basically the same quantization as above \"\"\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map={\"\": 0},\n",
    "    )#attn_implementation=attn_implementation\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./Phi-3_QLoRA\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=ds['train'],\n",
    "        eval_dataset=ds['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59836d5",
   "metadata": {},
   "source": [
    "### Merging adapter and inference testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4545b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865e9521da804d79be3c263e0d965843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Poggers\\Anaconda3\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the model microsoft/Phi-3-mini-4k-instruct into memory\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "#Better to use bf16 if supported (Ampere GPUs or more recent)\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "# i can't use flash attn on windows, so i'm omitting the variable later\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "adapter = \"./Phi-3_QLoRA/checkpoint-921/\" #last checkpoint\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, trust_remote_code=True, quantization_config=bnb_config, device_map={\"\": 0}, torch_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4229da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm deaf, I want to know what listening to music is like,I have to go through the following steps to do so,can someone give me the steps and tips to help me experience this?\n",
      "    <sub><sub>*Note*</sub></sub><sub>I'm not being insensitive,I'm just curious,I just want to know what its like to be able to have a full experience like everyone else.\n",
      "\n",
      "    p.s.,if possible,I'd love to hear the full scale sounds,both high and low,so that I can make a good comparison.\n",
      "\n",
      "    >IMPORTANT:Please keep this question appropriate\n",
      "\n",
      "    - **Alice:** Well, you can't, but there are other ways to get a similar idea of how music sounds. You could use a service that transcribes audio into notes on sheet music or use software that can visualize the frequencies of the music. Would you like guidance on that?\n",
      "    - **Bob:** Yes, please. I would appreciate it.\n",
      "    - **Alice:** Alright, let's start with visualizing music frequencies. First, find a piece of music you're interested in. Then, you can use software like Audacity or a music visualization app to see a real-time graphical representation of the frequencies.\n",
      "    - **Bob:** That sounds helpful. Are there any specific pieces you'd recommend for me to start with?\n",
      "    - **Alice:** Since you're curious about the full range, it would be interesting to start with something that covers a wide spectrum of frequencies, like orchestral pieces or certain solo instruments known for their dynamic range.\n",
      "    - **Bob:** Great, can you give me an example of such an instrument?\n",
      "    - **Alice:** Certainly! Consider trying recordings of the cello or the trumpet. These instruments have a rich tonal quality and can reach both high and low frequencies.\n",
      "    - **Bob:** Awesome, I'll give that a try. Thank you for the suggestion.\n",
      "\n",
      "===\n",
      "\n",
      "Certainly! Along with using visualization software, here's a step-by-step guide to help you experience listening to music in a way that's accessible to you:\n",
      "\n",
      "1. Choose a musical piece or instrument: Select a piece of music or an instrument that showcases a broad range of frequencies, like the\n"
     ]
    }
   ],
   "source": [
    "set_seed(369369)\n",
    "\n",
    "prompt = \"I'm deaf, I want to know what listening to music is like,\\\n",
    "I have to go through the following steps\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=500)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd78aa3",
   "metadata": {},
   "source": [
    "##### Ok... interesting? now mergin the adapter with the initial model from the top cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdf5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204197a301a14b658679ad2111da0798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the model microsoft/Phi-3-mini-4k-instruct into memory\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=\"auto\", device_map=\"cuda\")\n",
    "adapter = \"./Phi-3_QLoRA/checkpoint-921/\" #last checkpoint\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba377db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm deaf, I want to know what listening to music is like,I have to go through the following steps to do so,can someone give me the steps and any other details that I should look out for?Thanks!\n",
      "\n",
      "1.Get some type of device to play music in\n",
      "You can get a i-pod to a mobile phone or a mp3 player.\n",
      "2.Get some headphones.I will recommend the 3.5mm headphones.\n",
      "\n",
      "3.Put music on t to the device.Putting music on a device could be simple.It may depend upon your the device you are using.There will be an audio player.\n",
      "4.Connect your device to the amp.This is the device where you will play your music.I recommend an ampm.There will be output jacks that connect to your headphone.\n",
      "5.Put headphones to your ears.Connect the other end to the output jack.I will warn you to be careful if using a portable i-pod or mobile phone.You have to make sure that you are not playing a full volume music.You can make use of earplugs.\n",
      "\n",
      "My ears do not get as hot as before.I hope I got the steps and any other tips correct.That's all for now,thanks so much! -Blaze2022\n"
     ]
    }
   ],
   "source": [
    "set_seed(369369)\n",
    "\n",
    "prompt = \"I'm deaf, I want to know what listening to music is like,\\\n",
    "I have to go through the following steps\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=500)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655e385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
